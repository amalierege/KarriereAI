{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KarriereAI\n",
    "#### A deep learning model used to predict viable career paths to a user based on their skills and interests.\n",
    "\n",
    "#### Purpose\n",
    "KarriereAI will classify the appropriate career within technology for a user from an interactive quizlet input. The quizlet is used to determine the user's skills and interests, before the model will predict a fitting career based on the input data.\n",
    "\n",
    "#### Dataset\n",
    "The dataset to be used is from the paper Skill2vec: A Machine Learning Approach for Determining the Relevant Skills from Job Description, by Van-Duyet Le et al. <a href=\"https://arxiv.org/pdf/1707.09751\">here</a>. Containing relevant columns of job titles related to a free-text field of job descriptions describing relevant skills. \n",
    "\n",
    "#### Model Architecture\n",
    "The main model is a feed-forward neural network (FNN) for classification, using a multi-layer-perceptron (MLP) architecture suitable for structured data classification. \n",
    "\n",
    "Part of engineering the main model requires preprocessing of the dataset in a natural language processing (NLP) set-up, preparing it to run through a sub-model with an encoded transformer architecture.\n",
    "\n",
    "The main model also requires figuring out basic vs. deep MLP architecures. Additionally, figuring out whether the model should contain batch normalization, regularization and/or dropout. Lastly, experimenting with different activation functions.\n",
    "\n",
    "Through using the NLP sub-model, the data will be vectorized to work with the MLP classifier rather than the language model.\n",
    "\n",
    "#### Evaluation\n",
    "As far as evaluation goes, a confusion matrix and an F1 score will be computed along with standard evaluation metrics like accuracy, recall and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Importing Libraries and Loading the Data \n",
    "We will be needing different libraries from <a href=\"https://keras.io/api/\">Keras</a> and <a href=\"https://www.tensorflow.org/api_docs/python/tf\">TensorFlow</a> to make computations on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Model processing\n",
    "import sklearn\n",
    "import numpy\n",
    "import pandas \n",
    "import tensorflow\n",
    "import keras\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import layers, Sequential\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Other\n",
    "import datetime\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "import matplotlib as plot\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "from pathlib import Path\n",
    "import seaborn\n",
    "seaborn.set_theme (style = \"whitegrid\")\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing relevant libraries, we load the dataset we wish to train the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pandas.read_csv (\"data/mustHaveSkills.csv\", header = 0, encoding ='ISO-8859-1')\n",
    "del data ['job_brief_id']\n",
    "\n",
    "# REF: Van-Duyet Le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Taking a Look at the Data\n",
    "\n",
    "To create optimal and smooth-running Python for the model we want to  study the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (261724, 2) \n",
      "\n",
      "Information about data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261724 entries, 0 to 261723\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   keyword_name  261717 non-null  object\n",
      " 1   job_title     261724 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Basic information about dataset\n",
    "print (\"Shape of dataset:\", data.shape, \"\\n\")\n",
    "print (\"Information about dataset:\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 80810 entries, 0 to 261723\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   keyword_name  80803 non-null  object\n",
      " 1   job_title     80810 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "data = data.drop_duplicates (subset = ['keyword_name', 'job_title'], keep = 'last')\n",
    "data = data [data [\"job_title\"] != 0]\n",
    "print (data.info())\n",
    "\n",
    "# String magic\n",
    "data ['Count'] = data.groupby ('job_title')['keyword_name'].transform (pandas.Series.value_counts)\n",
    "data.drop_duplicates (inplace = True)\n",
    "data ['keyword_name'] = data ['keyword_name'].str.lower()\n",
    "data ['keyword_name'] = data ['keyword_name'].str.replace(' ', '_')\n",
    "data ['job_title'] = data ['job_title'].str.lower()\n",
    "\n",
    "# Clean data\n",
    "data_jobtitle = data.groupby ('job_title')['keyword_name'].apply(list)\n",
    "\n",
    "# REF: Van-Duyet Le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_title\n",
       ".net                             [microsoft_office_sharepoint_server]\n",
       ".net application                       [architect, .net, asp.net, c#]\n",
       ".net developer      [.net, sharepoint, moss, asp.net, html/html5, ...\n",
       ".net engineer       [.net, .net, asp.net, javascript, mobile_appli...\n",
       ".net programmer                                                [.net]\n",
       "                                          ...                        \n",
       "xbox security                            [c++, xbox, ps, playstation]\n",
       "xd                  [ux_design, ux, uxd, ued, xd, user_experience_...\n",
       "xml developer                                        [xml, html, css]\n",
       "yield manager       [yield, online_media, digital, advertising, br...\n",
       "zbb analysts           [zerbased_budgeting, zbb, accounting, finance]\n",
       "Name: keyword_name, Length: 4596, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at contents of dataset\n",
    "data_jobtitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Exploratory Data Analysis (EDA)\n",
    "\n",
    "To know what data the model will injest, taking a closer look by constructing plots and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Plotting Jobs and Skills\n",
    "##### 3.1.1 Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count number of occurrences \n",
    "def count_items(series):\n",
    "    items = series.dropna().apply(lambda x: x.split(\";\"))\n",
    "    flat_list = [item.strip() for sublist in items for item in sublist]\n",
    "    return pandas.Series(flat_list).value_counts()\n",
    "\n",
    "# REF: Adil Shamim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Jobs:\n",
      " software engineer                      1583\n",
      "software developer                      941\n",
      "engineer                                891\n",
      "manager                                 853\n",
      "developer                               663\n",
      "                                       ... \n",
      "js engineer                               1\n",
      "js programmer                             1\n",
      "industrial designer                       1\n",
      "third party marketing games manager       1\n",
      "marketing games manager                   1\n",
      "Name: count, Length: 4597, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count Jobs\n",
    "job_count = count_items(data[\"job_title\"])\n",
    "print(\"Most Common Jobs:\\n\", job_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Skills:\n",
      " java                          771\n",
      "javascript                    500\n",
      "python                        473\n",
      "c++                           447\n",
      "sql                           361\n",
      "                             ... \n",
      "mdd                             1\n",
      "clinical_evaluation_report      1\n",
      "cer                             1\n",
      "vaccines                        1\n",
      "actuate_report                  1\n",
      "Name: count, Length: 6876, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count Skills\n",
    "skills_count = count_items(data[\"keyword_name\"])\n",
    "print(\"Most Common Skills:\\n\", skills_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Preprocessing of Data\n",
    "\n",
    "For the purpose of cleaning the dataset to make sure the model doesn't learn errors making predictions skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Missing Values\n",
    "Checking for missing values and dropping them if they few enough to make less of an impact. Luckily for us, there are no missing values in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_values = data.isnull().sum()\n",
    "# print ('Missing Values in Each Column')\n",
    "# print (missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Feature Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to be used\n",
    "features = data [['job_title', 'keyword_name']]\n",
    "target = data ['job_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Process Text Fields\n",
    "Turning skills and interests into vectorized numerical features for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for vector_list in data_jobtitle:\n",
    "    vlist = list (set (vector_list))\n",
    "    if vlist not in data:\n",
    "        data.append (vlist)\n",
    "        \n",
    "len (data)\n",
    "data [0:2]\n",
    "\n",
    "# REF: Van-Duyet Le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(...)? (4144046653.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint len(data_naruki_final)\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m Missing parentheses in call to 'print'. Did you mean print(...)?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_naruki = pd.read_csv('naukri_skill_full', header = 0, encoding='ISO-8859-1')\n",
    "# drop duplicate\n",
    "data_naruki.drop_duplicates(subset=['id', 'skill'], keep='last')\n",
    "# lower-case\n",
    "data_naruki['skill'] = data_naruki['skill'].str.lower()\n",
    "data_naruki['skill'] = data_naruki['skill'].str.replace(' ','_')\n",
    "data_naruki_final = data_naruki.groupby('id')['skill'].apply(list)\n",
    "print (len (data_naruki_final))\n",
    "data_naruki_final[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of text features: (200, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arege\\AppData\\Local\\Temp\\ipykernel_1116\\2984523955.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features['Text_Features'] = features ['Skills'].fillna('') + ' ' + features ['Interests'].fillna('')\n",
      "c:\\Users\\arege\\Documents\\Dataingenior Bachelor\\DAT255\\KarriereAI\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Creating a combined text column\n",
    "features['Text_Features'] = features ['Skills'].fillna('') + ' ' + features ['Interests'].fillna('')\n",
    "\n",
    "# Text Feature Extraction\n",
    "vectorizer = CountVectorizer (tokenizer = lambda x: [item.strip() for item in x.split(\";\") if item.strip()], lowercase = True)\n",
    "\n",
    "text_features = vectorizer.fit_transform(features['Text_Features'])\n",
    "\n",
    "# # Text based matrix from Skills\n",
    "# skills_matrix = vectorizer.fit_transform (features ['Skills'])\n",
    "\n",
    "# # Text based matrix from Interests\n",
    "# interests_matrix = vectorizer.fit_transform (features ['Interests'])\n",
    "\n",
    "# # Numeric matrix from Age\n",
    "# age_matrix = features['Age'].values\n",
    "\n",
    "# skills_and_interests_matrix = vectorizer.fit_transform (features ['Skills_and_Interests'])\n",
    "print (\"Shape of text features:\", text_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the BERT vocabulary generation method from Tensorflow Text\n",
    "\n",
    "bert_tokenizer_params = dict (lower_case = True)\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    \n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens = reserved_tokens,\n",
    "    \n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params = bert_tokenizer_params,\n",
    "    \n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params = {},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_pt.batch (1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pt_vocab[:10])\n",
    "print(pt_vocab[100:110])\n",
    "print(pt_vocab[1000:1010])\n",
    "print(pt_vocab[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file (filepath, vocab):\n",
    "  with open(filepath, 'w') as f:\n",
    "    for token in vocab:\n",
    "      print(token, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file ('pt_vocab.txt', pt_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Tokenize Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the BERT tokenizer on the newly generated vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Constructing the Feature Matrix\n",
    "\n",
    "Combining the numerical value of Age with the vectorized Skills and Interest features into a sparse matrix concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (200, 151)\n",
      "Target Classes: ['AI Researcher' 'AI Specialist' 'Automation Engineer' 'Backend Developer'\n",
      " 'Biostatistician' 'Business Analyst' 'Cloud Engineer'\n",
      " 'Content Strategist' 'Cybersecurity Analyst' 'Cybersecurity Specialist'\n",
      " 'Data Analyst' 'Data Engineer' 'Data Scientist' 'Deep Learning Engineer'\n",
      " 'DevOps Engineer' 'Digital Marketer' 'Embedded Systems Engineer'\n",
      " 'Financial Analyst' 'Front-end Developer' 'Full Stack Developer'\n",
      " 'Graphic Designer' 'Machine Learning Engineer' 'Marketing Manager'\n",
      " 'Mobile Developer' 'NLP Engineer' 'Project Manager' 'Research Analyst'\n",
      " 'Research Scientist' 'Software Developer' 'Software Engineer'\n",
      " 'UX Designer' 'UX Researcher']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Numeric matrix from Age\n",
    "age_matrix = features[['Age']].values # ? Updated to double-bracket\n",
    "\n",
    "# Combining the matrices\n",
    "feature_matrix = hstack ([sparse.csr_matrix (age_matrix), text_features]) # X\n",
    "coo = feature_matrix.tocoo()\n",
    "indices = numpy.asmatrix([coo.row, coo.col]).transpose()\n",
    "feature_matrix = tensorflow.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "# Encoding target variable\n",
    "target_encoder = LabelEncoder ()\n",
    "encoded_target = target_encoder.fit_transform (target) # Y\n",
    "\n",
    "print (\"Feature Matrix Shape:\", feature_matrix.shape) # X\n",
    "print (\"Target Classes:\", target_encoder.classes_) # Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Building the Model\n",
    "\n",
    "Building the FFN model, and training it on training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Training, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print (type(feature_matrix), type(encoded_target))\n",
    "\n",
    "X_train = feature_matrix \n",
    "y_train = encoded_target \n",
    "\n",
    "X_train = tensorflow.sparse.reorder(X_train)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split (feature_matrix, encoded_target, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# tensorflow.sparse.reorder(X_train)\n",
    "\n",
    "# this is trash:\n",
    "# training_set, validation_set, test_set = train_val_test_split (age_matrix, feature_matrix, )\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous logs\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.0437 - loss: 0.5685 - val_accuracy: 0.0250 - val_loss: 0.0411\n",
      "Epoch 2/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0378 - loss: 0.0684 - val_accuracy: 0.0250 - val_loss: 0.0181\n",
      "Epoch 3/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0841 - loss: 0.0267 - val_accuracy: 0.0250 - val_loss: 0.0108\n",
      "Epoch 4/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0272 - loss: 0.0433 - val_accuracy: 0.0000e+00 - val_loss: 0.0097\n",
      "Epoch 5/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0524 - loss: 0.0202 - val_accuracy: 0.0250 - val_loss: 0.0147\n",
      "Epoch 6/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0425 - loss: 0.0249 - val_accuracy: 0.0000e+00 - val_loss: 0.0098\n",
      "Epoch 7/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0897 - loss: 0.0266 - val_accuracy: 0.0000e+00 - val_loss: 0.0153\n",
      "Epoch 8/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0141 - loss: 0.0606 - val_accuracy: 0.0250 - val_loss: 0.0060\n",
      "Epoch 9/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0368 - loss: 0.0936 - val_accuracy: 0.0250 - val_loss: 0.0106\n",
      "Epoch 10/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0261 - loss: 0.0334 - val_accuracy: 0.0250 - val_loss: 0.0096\n",
      "Epoch 11/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0102 - loss: 0.0270 - val_accuracy: 0.0250 - val_loss: 0.0159\n",
      "Epoch 12/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0233 - loss: 0.0192 - val_accuracy: 0.0250 - val_loss: 0.0166\n",
      "Epoch 13/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0212 - loss: 0.0291 - val_accuracy: 0.0250 - val_loss: 0.0086\n",
      "Epoch 14/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0399 - loss: 0.0262 - val_accuracy: 0.0250 - val_loss: 0.0156\n",
      "Epoch 15/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0063 - loss: 0.0136 - val_accuracy: 0.0500 - val_loss: 0.0206\n",
      "Epoch 16/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0218 - loss: 0.0403 - val_accuracy: 0.0250 - val_loss: 0.0048\n",
      "Epoch 17/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0079 - loss: 0.0470 - val_accuracy: 0.0250 - val_loss: 0.0205\n",
      "Epoch 18/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0192 - loss: 0.0816 - val_accuracy: 0.0250 - val_loss: 0.0128\n",
      "Epoch 19/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0127 - loss: 0.0425 - val_accuracy: 0.0250 - val_loss: 0.0073\n",
      "Epoch 20/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0412 - loss: 0.0132 - val_accuracy: 0.0750 - val_loss: 0.0204\n",
      "Epoch 21/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0123 - loss: 0.0490 - val_accuracy: 0.0750 - val_loss: 0.0214\n",
      "Epoch 22/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0376 - loss: 0.0322 - val_accuracy: 0.0250 - val_loss: 0.0242\n",
      "Epoch 23/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0388 - loss: 0.0261 - val_accuracy: 0.0250 - val_loss: 0.0244\n",
      "Epoch 24/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0070 - loss: 0.0333 - val_accuracy: 0.0000e+00 - val_loss: 0.0321\n",
      "Epoch 25/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0380 - loss: 0.0643 - val_accuracy: 0.0250 - val_loss: 0.0074\n",
      "Epoch 26/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0169 - loss: 0.0530 - val_accuracy: 0.0250 - val_loss: 0.0175\n",
      "Epoch 27/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0299 - loss: 0.0084 - val_accuracy: 0.0000e+00 - val_loss: 0.0328\n",
      "Epoch 28/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0155 - loss: 0.0326 - val_accuracy: 0.0500 - val_loss: 0.0150\n",
      "Epoch 29/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0068 - loss: 0.0275 - val_accuracy: 0.0250 - val_loss: 0.0177\n",
      "Epoch 30/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0380 - loss: 0.0152 - val_accuracy: 0.0500 - val_loss: 0.0156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27e43a62c50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    keras.Input (shape = (feature_matrix.shape[1], )),\n",
    "    # to complete the embedding, a one-hot layer:layers.StringLookup (output_mode = \"one-hot\")\n",
    "    layers.Dropout (0.1),\n",
    "    layers.Dense (16, activation = 'relu'),\n",
    "    layers.Dense (16, activation = 'relu'),\n",
    "    # followed by a Dense layer: layers.Dense (units = embedding_dim, use_bias = False, activation = None)\n",
    "    layers.Dense (target.unique().size)\n",
    "])\n",
    "\n",
    "# model.compile(loss = 'SparseCategoricalCrossentropy', optimizer = 'adam')\n",
    "model.compile (loss = 'hinge', \n",
    "              optimizer = 'adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)\n",
    "\n",
    "model.fit (X_train, y_train, \n",
    "          epochs = 30, \n",
    "          batch_size = 1, \n",
    "          validation_split = 0.2,\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">151</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m151\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m544\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,378</span> (44.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,378\u001b[0m (44.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,792</span> (14.81 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,792\u001b[0m (14.81 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,586</span> (29.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m7,586\u001b[0m (29.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 20476), started 1 day, 13:53:40 ago. (Use '!kill 20476' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-42f90d4aacb34269\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-42f90d4aacb34269\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# y_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(y_train, y_pred)\n",
    "# plot.figure(figsize = (10, 8))\n",
    "# seaborn.heatmap(cm, annot = True, fmt = \"d\", cmap = \"Blues\",\n",
    "#             xticklabels = target_encoder.classes_,\n",
    "#             yticklabels = target_encoder.classes_)\n",
    "# plot.xlabel(\"Predicted\")\n",
    "# plot.ylabel(\"Actual\")\n",
    "# plot.title(\"Confusion Matrix\")\n",
    "# plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Precision, Accuracy, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
